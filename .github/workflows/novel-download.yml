name: 在线下载小说

on:
  workflow_dispatch:
    inputs:
      novel_id:
        description: '小说ID (从番茄小说URL中获取)'
        required: true
      threads:
        description: '下载线程数 (1-10)'
        required: true
        default: '5'

permissions:
  contents: read
  actions: write

jobs:
  download-novel:
    runs-on: ubuntu-latest
    steps:
    - name: 检出代码
      uses: actions/checkout@v3

    - name: 设置Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'

    - name: 安装依赖
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt ebooklib

    - name: 确保cookie.json存在
      run: |
        if [ ! -f "./cookie.json" ]; then
          echo '""' > "./cookie.json"
        fi

    - name: 安装虚拟显示服务
      run: |
        sudo apt-get update
        sudo apt-get install -y xvfb

    - name: 调整线程数
      run: |
        threads=${{ github.event.inputs.threads }}
        if [ $threads -gt 10 ]; then
          threads=10
        fi
        echo "threads=$threads" >> $GITHUB_ENV

    - name: 准备下载脚本
      run: |
        cat > download_novel.py << 'EOF'
        import sys
        import os
        import time
        import requests
        import bs4
        import re
        import json
        import random
        from concurrent.futures import ThreadPoolExecutor, as_completed
        from collections import OrderedDict
        from ebooklib import epub

        # 小说ID和保存路径
        novel_id = sys.argv[1]
        threads_count = int(sys.argv[2])
        save_path = "novel_output"

        # 确保输出目录存在
        os.makedirs(save_path, exist_ok=True)

        # 配置
        CONFIG = {
            "max_workers": threads_count,
            "max_retries": 3,
            "request_timeout": 15,
            "status_file": "chapter.json",
            "user_agents": [
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0",
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
            ]
        }

        def get_headers(cookie=None):
            """生成随机请求头"""
            return {
                "User-Agent": random.choice(CONFIG["user_agents"]),
                "Cookie": cookie if cookie else get_cookie()
            }

        def get_cookie():
            """生成或加载Cookie"""
            cookie_path = "cookie.json"
            if os.path.exists(cookie_path):
                try:
                    with open(cookie_path, 'r') as f:
                        return json.load(f)
                except:
                    pass

            for _ in range(10):
                novel_web_id = random.randint(10**18, 10**19-1)
                cookie = f'novel_web_id={novel_web_id}'
                try:
                    resp = requests.get(
                        'https://fanqienovel.com',
                        headers={"User-Agent": random.choice(CONFIG["user_agents"])},
                        cookies={"novel_web_id": str(novel_web_id)},
                        timeout=10
                    )
                    if resp.ok:
                        with open(cookie_path, 'w') as f:
                            json.dump(cookie, f)
                        return cookie
                except Exception as e:
                    print(f"Cookie生成失败: {str(e)}")
                    time.sleep(0.5)
            raise Exception("无法获取有效Cookie")

        def down_text(it):
            """下载章节内容"""
            max_retries = CONFIG.get('max_retries', 3)
            retry_count = 0
            content = ""

            while retry_count < max_retries:
                try:
                    api_url = f"https://api.cenguigui.cn/api/tomato/content.php?item_id={it}"
                    response = requests.get(api_url, timeout=CONFIG["request_timeout"])
                    data = response.json()

                    if data.get("code") == 200:
                        content = data.get("data", {}).get("content", "")
                        content = re.sub(r'<header>.*?</header>', '', content, flags=re.DOTALL)
                        content = re.sub(r'<footer>.*?</footer>', '', content, flags=re.DOTALL)
                        content = re.sub(r'</?article>', '', content)
                        content = re.sub(r'<p idx="\d+">', '\n', content)
                        content = re.sub(r'</p>', '\n', content)
                        content = re.sub(r'<[^>]+>', '', content)
                        content = re.sub(r'\\u003c|\\u003e', '', content)

                        title = data.get("data", {}).get("title", "")
                        if title and content.startswith(title):
                            content = content[len(title):].lstrip()

                        content = re.sub(r'\n{2,}', '\n', content).strip()
                        content = '\n'.join(['    ' + line if line.strip() else line for line in content.split('\n')])
                        break
                except Exception as e:
                    print(f"请求失败: {str(e)}, 重试第{retry_count + 1}次...")
                    retry_count += 1
                    time.sleep(1 * retry_count)

            return content

        def get_book_info(book_id, headers):
            """获取书名、作者、简介"""
            url = f'https://fanqienovel.com/page/{book_id}'
            response = requests.get(url, headers=headers)
            if response.status_code != 200:
                print(f"网络请求失败，状态码: {response.status_code}")
                return None, None, None

            soup = bs4.BeautifulSoup(response.text, 'html.parser')
            name_element = soup.find('h1')
            name = name_element.text if name_element else "未知书名"

            author_name_element = soup.find('div', class_='author-name')
            author_name = None
            if author_name_element:
                author_name_span = author_name_element.find('span', class_='author-name-text')
                author_name = author_name_span.text if author_name_span else "未知作者"

            description_element = soup.find('div', class_='page-abstract-content')
            description = None
            if description_element:
                description_p = description_element.find('p')
                description = description_p.text if description_p else "无简介"

            return name, author_name, description

        def extract_chapters(soup):
            """解析章节列表"""
            chapters = []
            for idx, item in enumerate(soup.select('div.chapter-item')):
                a_tag = item.find('a')
                if not a_tag:
                    continue

                raw_title = a_tag.get_text(strip=True)
                if re.match(r'^(番外|特别篇|if线)\s*', raw_title):
                    final_title = raw_title
                else:
                    clean_title = re.sub(r'^第[一二三四五六七八九十百千\d]+章\s*', '', raw_title).strip()
                    final_title = f"第{idx+1}章 {clean_title}"

                chapters.append({
                    "id": a_tag['href'].split('/')[-1],
                    "title": final_title,
                    "url": f"https://fanqienovel.com{a_tag['href']}",
                    "index": idx
                })

            return chapters

        def download_novel(book_id, save_path):
            """下载小说的主函数"""
            try:
                headers = get_headers()
                print("正在获取书籍信息...")
                name, author_name, description = get_book_info(book_id, headers)
                if not name:
                    raise Exception("无法获取书籍信息，请检查小说ID或网络连接")

                print(f"书名：《{name}》")
                print(f"作者：{author_name}")
                print(f"简介：{description}")

                url = f'https://fanqienovel.com/page/{book_id}'
                response = requests.get(url, headers=headers)
                soup = bs4.BeautifulSoup(response.text, 'html.parser')

                chapters = extract_chapters(soup)
                if not chapters:
                    raise Exception("未找到任何章节")

                print(f"\n开始下载，共 {len(chapters)} 章")
                os.makedirs(save_path, exist_ok=True)

                output_file = os.path.join(save_path, f"{name}.txt")
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(f"书名：《{name}》\n作者：{author_name}\n\n简介：\n{description}\n\n")

                total_chapters = len(chapters)
                success_count = 0
                downloaded_chapters = set()
                content_cache = OrderedDict()

                for chapter in chapters[:5]:
                    content = down_text(chapter["id"])
                    if content:
                        content_cache[chapter["index"]] = (chapter, content)
                        downloaded_chapters.add(chapter["id"])
                        success_count += 1
                        progress = (success_count / total_chapters) * 100
                        print(f"进度: {progress:.2f}% - 正在下载: {success_count}/{total_chapters}")
                        print(f"已下载：{chapter['title']}")

                remaining_chapters = chapters[5:]
                with ThreadPoolExecutor(max_workers=CONFIG["max_workers"]) as executor:
                    future_to_chapter = {
                        executor.submit(down_text, chapter["id"]): chapter
                        for chapter in remaining_chapters
                    }

                    for future in as_completed(future_to_chapter):
                        chapter = future_to_chapter[future]
                        try:
                            content = future.result()
                            if content:
                                content_cache[chapter["index"]] = (chapter, content)
                                downloaded_chapters.add(chapter["id"])
                                success_count += 1
                                print(f"已下载：{chapter['title']}")
                        except Exception as e:
                            print(f"下载失败：{chapter['title']} - {str(e)}")
                        finally:
                            progress = (success_count / total_chapters) * 100
                            print(f"进度: {progress:.2f}% - 正在下载: {success_count}/{total_chapters}")

                print("\n正在保存文件...")
                processed_contents = set()
                with open(output_file, 'a', encoding='utf-8') as f:
                    for index in sorted(content_cache.keys()):
                        chapter, content = content_cache[index]
                        content_hash = hash(content)
                        if content_hash in processed_contents:
                            print(f"跳过重复章节：{chapter['title']}")
                            continue
                        processed_contents.add(content_hash)
                        f.write(f"\n{chapter['title']}\n\n")
                        f.write(content + "\n\n")

                # Generate EPUB
                print("正在生成EPUB文件...")
                book = epub.EpubBook()
                book.set_identifier(f'novel_{book_id}')
                book.set_title(name)
                book.set_language('zh')
                book.add_author(author_name)

                # Add introduction chapter
                intro_chapter = epub.EpubHtml(title='简介', file_name='intro.xhtml', lang='zh')
                intro_chapter.content = f'<h1>简介</h1><p>{description}</p>'
                book.add_item(intro_chapter)
                epub_chapters = [intro_chapter]

                # Add chapters
                for index in sorted(content_cache.keys()):
                    chapter, content = content_cache[index]
                    epub_chapter = epub.EpubHtml(title=chapter['title'], file_name=f'chap_{index}.xhtml', lang='zh')
                    html_content = f'<h1>{chapter["title"]}</h1>'
                    paragraphs = content.split('\n')
                    for para in paragraphs:
                        if para.strip():
                            html_content += f'<p>{para.strip()}</p>'
                    epub_chapter.content = html_content
                    book.add_item(epub_chapter)
                    epub_chapters.append(epub_chapter)

                # Define spine and toc
                book.spine = ['nav'] + epub_chapters
                book.toc = epub_chapters
                book.add_item(epub.EpubNcx())
                book.add_item(epub.EpubNav())

                # Write EPUB file
                epub_file = os.path.join(save_path, f"{name}.epub")
                epub.write_epub(epub_file, book)
                print(f"EPUB文件已保存: {epub_file}")

                print(f"\n下载完成！成功：{success_count}章，失败：{total_chapters - success_count}章")
                print(f"文件保存在：{output_file} 和 {epub_file}")
                return True

            except Exception as e:
                print(f"\n错误：{str(e)}")
                print(f"下载失败: {str(e)}")
                return False

        print(f"开始下载小说 ID: {novel_id}")
        print(f"保存路径: {save_path}")
        print(f"使用线程数: {threads_count}")

        success = download_novel(novel_id, save_path)

        if success:
            print("下载完成！")
            print("\n下载的文件列表:")
            for file in os.listdir(save_path):
                file_path = os.path.join(save_path, file)
                file_size = os.path.getsize(file_path) / 1024
                print(f"- {file} ({file_size:.2f} KB)")
        else:
            print("下载失败，请检查错误信息")
            sys.exit(1)
        EOF

    - name: 下载小说
      run: |
        echo "开始下载小说..."
        python download_novel.py "${{ github.event.inputs.novel_id }}" "${{ env.threads }}"

    - name: 压缩下载结果
      run: |
        cd novel_output && zip -r ../novel_files.zip *

    - name: 上传下载结果
      uses: actions/upload-artifact@v4
      with:
        name: novel-${{ github.event.inputs.novel_id }}-files
        path: novel_files.zip
        retention-days: 7

    - name: 提供下载信息
      run: |
        echo "✅ 小说下载完成！"
        echo "请点击上方 'Summary' 标签，然后在 'Artifacts' 部分下载小说文件（包含TXT和EPUB格式）。"
        echo "文件保存期限为7天。"
